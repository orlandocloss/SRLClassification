{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare challenge dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are taking the chatgpt generated examples (and the special case manual examples). And preparing in a json file.\n",
    "\n",
    " Organized hierarchically under \"capabilities\" that represent distinct linguistic phenomena (distance, spatial/temporal distinctions, dative alternations, negation, head noun identification, and predicate disambiguation), the dataset employs four testing formats: MFT (Minimal Functional Test) for single-sentence evaluations, DIR (Directional) for paired sentences with different expected labels, INV (Invariant) for paired sentences with consistent labeling, and specialDIR for specialized predicate positioning tests. Each example contains tokenized sentences with specified predicate and argument indices along with their expected semantic role labels (like ARG0, ARG2, ARGM-TMP, ARGM-LOC), allowing for systematic assessment of a model's understanding of semantic roles across syntactic variations and linguistic challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we store tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "MFT:(tokenized, predicate_index, target_index, target_SRL) \n",
    "DIR:(tokenized1,tokenized2,predicate_index,target_index1, target_index2, target_SRL1, target_SRL2), \n",
    "INV: (tokenized1,tokenized2,predicate_index,target_index1,target_index2, target_SRL)\n",
    "specialDIR: (tokenized1, tokenized2, predicate_index1, predicate_index2, SRL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"version\": \"1.0\",\n",
    "    \"description\": \"Semantic Role Labeling Challenge Dataset\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_examples(dataset, capability, test_type, test_name, *args):\n",
    "    \"\"\"\n",
    "    Adds examples to the dataset directly from lists of data.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: The dataset dictionary to add examples to\n",
    "    - capability: String identifying the capability (e.g., 'distance', 'spacetemp')\n",
    "    - test_type: String indicating the format ('MFT', 'DIR', 'INV', 'specialDIR')\n",
    "    - test_name: String providing a unique name for this specific test\n",
    "    - *args: Lists of data appropriate for the test type (see function body for details)\n",
    "    \n",
    "    Returns:\n",
    "    - The updated dataset dictionary\n",
    "    \"\"\"\n",
    "    if \"capabilities\" not in dataset:\n",
    "        dataset[\"capabilities\"] = {}\n",
    "    \n",
    "    if capability not in dataset[\"capabilities\"]:\n",
    "        dataset[\"capabilities\"][capability] = {}\n",
    "    \n",
    "    if test_type not in dataset[\"capabilities\"][capability]:\n",
    "        dataset[\"capabilities\"][capability][test_type] = {}\n",
    "    \n",
    "    if test_name not in dataset[\"capabilities\"][capability][test_type]:\n",
    "        dataset[\"capabilities\"][capability][test_type][test_name] = []\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    if test_type == \"MFT\":\n",
    "        # For MFT: add_examples(dataset, \"capability\", \"MFT\", tokenized_list, predicate_indexes, target_indexes, target_srls)\n",
    "        if len(args) != 4:\n",
    "            raise ValueError(\"MFT format requires 4 lists: tokenized, predicate_indexes, target_indexes, target_srls\")\n",
    "        \n",
    "        tokenized_list, predicate_indexes, target_indexes, target_srls = args\n",
    "        \n",
    "        for i in range(len(tokenized_list)):\n",
    "            example = {\n",
    "                \"tokenized\": tokenized_list[i],\n",
    "                \"predicate_index\": predicate_indexes[i],\n",
    "                \"target_index\": target_indexes[i],\n",
    "                \"target_SRL\": target_srls[i]\n",
    "            }\n",
    "            examples.append(example)\n",
    "    \n",
    "    elif test_type == \"DIR\":\n",
    "        # For DIR: add_examples(dataset, \"capability\", \"DIR\", tokenized_list1, tokenized_list2, \n",
    "        #                      predicate_indexes, target_indexes1, target_indexes2, target_srls1, target_srls2)\n",
    "        if len(args) != 7:\n",
    "            raise ValueError(\"DIR format requires 7 lists: tokenized1, tokenized2, predicate_indexes, \" +\n",
    "                           \"target_indexes1, target_indexes2, target_srls1, target_srls2\")\n",
    "        \n",
    "        tokenized_list1, tokenized_list2, predicate_indexes, target_indexes1, target_indexes2, target_srls1, target_srls2 = args\n",
    "        \n",
    "        for i in range(min(len(tokenized_list1), len(tokenized_list2))):\n",
    "            example = {\n",
    "                \"tokenized1\": tokenized_list1[i],\n",
    "                \"tokenized2\": tokenized_list2[i],\n",
    "                \"predicate_index\": predicate_indexes[i],\n",
    "                \"target_index1\": target_indexes1[i],\n",
    "                \"target_index2\": target_indexes2[i],\n",
    "                \"target_SRL1\": target_srls1[i],\n",
    "                \"target_SRL2\": target_srls2[i]\n",
    "            }\n",
    "            examples.append(example)\n",
    "    \n",
    "    elif test_type == \"INV\":\n",
    "        # For INV: add_examples(dataset, \"capability\", \"INV\", tokenized_list1, tokenized_list2, \n",
    "        #                      predicate_indexes, target_indexes1, target_indexes2, target_srls)\n",
    "        if len(args) != 6:\n",
    "            raise ValueError(\"INV format requires 6 lists: tokenized1, tokenized2, predicate_indexes, \" +\n",
    "                           \"target_indexes1, target_indexes2, target_srls\")\n",
    "        \n",
    "        tokenized_list1, tokenized_list2, predicate_indexes, target_indexes1, target_indexes2, target_srls = args\n",
    "        \n",
    "        for i in range(min(len(tokenized_list1), len(tokenized_list2))):\n",
    "            example = {\n",
    "                \"tokenized1\": tokenized_list1[i],\n",
    "                \"tokenized2\": tokenized_list2[i],\n",
    "                \"predicate_index\": predicate_indexes[i],\n",
    "                \"target_index1\": target_indexes1[i],\n",
    "                \"target_index2\": target_indexes2[i],\n",
    "                \"target_SRL\": target_srls[i]\n",
    "            }\n",
    "            examples.append(example)\n",
    "    \n",
    "    elif test_type == \"specialDIR\":\n",
    "        # For specialDIR: add_examples(dataset, \"capability\", \"specialDIR\", tokenized_list1, tokenized_list2, \n",
    "        #                             predicate_indexes1, predicate_indexes2, srls)\n",
    "        if len(args) != 5:\n",
    "            raise ValueError(\"specialDIR format requires 5 lists: tokenized1, tokenized2, \" +\n",
    "                           \"predicate_indexes1, predicate_indexes2, srls\")\n",
    "        \n",
    "        tokenized_list1, tokenized_list2, predicate_indexes1, predicate_indexes2, srls = args\n",
    "        \n",
    "        for i in range(min(len(tokenized_list1), len(tokenized_list2))):\n",
    "            example = {\n",
    "                \"tokenized1\": tokenized_list1[i],\n",
    "                \"tokenized2\": tokenized_list2[i],\n",
    "                \"predicate_index1\": predicate_indexes1[i],\n",
    "                \"predicate_index2\": predicate_indexes2[i],\n",
    "                \"SRL\": srls[i]\n",
    "            }\n",
    "            examples.append(example)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown test type: {test_type}\")\n",
    "    \n",
    "    dataset[\"capabilities\"][capability][test_type][test_name].extend(examples)\n",
    "    \n",
    "    print(f\"Added {len(examples)} examples for capability '{capability}' with test type '{test_type}' and name '{test_name}'\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFT: Test assignment of ARG0 when a parenthetical phrase separates the subject and predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('distance1.csv')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def adjust_indices(sentence, original_indices):\n",
    "    '''\n",
    "    This function adjusts word indices from a simple space-based tokenization to match the indices of tokens\n",
    "    generated by the spaCy tokenizer. It takes a sentence and a list of original indices as input. The function\n",
    "    first tokenizes the sentence using both space-based and spaCy tokenization methods. It then maps the original\n",
    "    indices to spaCy token indices by finding the best character position overlap between the two tokenizations.\n",
    "    This is necessary because spaCy's tokenization can differ from simple space-based tokenization, affecting\n",
    "    the alignment of indices. The function returns a list of adjusted indices that correspond to the spaCy tokens.\n",
    "    '''\n",
    "    original_tokens = sentence.split()\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    original_char_spans = []\n",
    "    current_pos = 0\n",
    "    for token in original_tokens:\n",
    "        token_pos = sentence[current_pos:].find(token) + current_pos\n",
    "        original_char_spans.append((token_pos, token_pos + len(token)))\n",
    "        current_pos = token_pos + len(token)\n",
    "    \n",
    "    adjusted_indices = []\n",
    "    for idx in original_indices:\n",
    "        if 0 <= idx < len(original_char_spans):\n",
    "            orig_start, orig_end = original_char_spans[idx]\n",
    "            \n",
    "            best_token_idx = None\n",
    "            best_overlap = -1\n",
    "            \n",
    "            for i, token in enumerate(doc):\n",
    "                token_start = token.idx\n",
    "                token_end = token_start + len(token.text)\n",
    "                \n",
    "                overlap_start = max(orig_start, token_start)\n",
    "                overlap_end = min(orig_end, token_end)\n",
    "                overlap = max(0, overlap_end - overlap_start)\n",
    "                \n",
    "                if overlap > best_overlap:\n",
    "                    best_overlap = overlap\n",
    "                    best_token_idx = i\n",
    "            \n",
    "            if best_token_idx is not None:\n",
    "                adjusted_indices.append(best_token_idx)\n",
    "            else:\n",
    "                adjusted_indices.append(idx)\n",
    "        else:\n",
    "            adjusted_indices.append(idx)\n",
    "            \n",
    "    return adjusted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Original_ARG0_index</th>\n",
       "      <th>Original_Predicate_index</th>\n",
       "      <th>Adjusted_ARG0_index</th>\n",
       "      <th>Adjusted_Predicate_index</th>\n",
       "      <th>SpaCy_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jack, despite having an extremely busy schedul...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>[Jack, ,, despite, having, an, extremely, busy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mary, although feeling under the weather, prep...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>[Mary, ,, although, feeling, under, the, weath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My neighbor, who recently moved in, mowed the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>[My, neighbor, ,, who, recently, moved, in, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The teacher, after grading a mountain of paper...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[The, teacher, ,, after, grading, a, mountain,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob, having finished his homework early, watch...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>[Bob, ,, having, finished, his, homework, earl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Original_ARG0_index  \\\n",
       "0  Jack, despite having an extremely busy schedul...                    0   \n",
       "1  Mary, although feeling under the weather, prep...                    0   \n",
       "2  My neighbor, who recently moved in, mowed the ...                    1   \n",
       "3  The teacher, after grading a mountain of paper...                    1   \n",
       "4  Bob, having finished his homework early, watch...                    0   \n",
       "\n",
       "   Original_Predicate_index  Adjusted_ARG0_index  Adjusted_Predicate_index  \\\n",
       "0                        10                    0                        12   \n",
       "1                         6                    0                         8   \n",
       "2                         6                    1                         8   \n",
       "3                         8                    1                        10   \n",
       "4                         6                    0                         8   \n",
       "\n",
       "                                        SpaCy_Tokens  \n",
       "0  [Jack, ,, despite, having, an, extremely, busy...  \n",
       "1  [Mary, ,, although, feeling, under, the, weath...  \n",
       "2  [My, neighbor, ,, who, recently, moved, in, ,,...  \n",
       "3  [The, teacher, ,, after, grading, a, mountain,...  \n",
       "4  [Bob, ,, having, finished, his, homework, earl...  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row['Sentence']\n",
    "    arg0_index = row['ARG0_Index']\n",
    "    pred_index = row['Predicate_Index']\n",
    "    \n",
    "    # Adjust indices\n",
    "    adjusted_indices = adjust_indices(sentence, [arg0_index, pred_index])\n",
    "    \n",
    "    adjusted_data.append({\n",
    "        'Sentence': sentence,\n",
    "        'Original_ARG0_index': arg0_index,\n",
    "        'Original_Predicate_index': pred_index,\n",
    "        'Adjusted_ARG0_index': adjusted_indices[0],\n",
    "        'Adjusted_Predicate_index': adjusted_indices[1],\n",
    "        'SpaCy_Tokens': [token.text for token in nlp(sentence)]\n",
    "    })\n",
    "\n",
    "# Create new DataFrame with adjusted indices\n",
    "distance1 = pd.DataFrame(adjusted_data)\n",
    "\n",
    "# Display the results\n",
    "distance1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hailey'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the first row column SpaCy_tokens\n",
    "first_row_spacy_tokens = distance1.iloc[85]['SpaCy_Tokens']\n",
    "first_row_spacy_tokens[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 86 examples for capability 'distance' with test type 'MFT' and name '1'\n"
     ]
    }
   ],
   "source": [
    "tokenized_list = [row[\"SpaCy_Tokens\"] for _, row in distance1.iterrows()]\n",
    "predicate_indexes = [row[\"Adjusted_Predicate_index\"] for _, row in distance1.iterrows()]\n",
    "target_indexes = [row[\"Adjusted_ARG0_index\"] for _, row in distance1.iterrows()]\n",
    "target_srls = [\"ARG0\"] * len(tokenized_list)  \n",
    "\n",
    "dataset = add_examples(\n",
    "    dataset,\n",
    "    \"distance\",\n",
    "    \"MFT\",\n",
    "    \"1\",\n",
    "    tokenized_list,\n",
    "    predicate_indexes,\n",
    "    target_indexes,\n",
    "    target_srls\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFT: Test consistent ARG0 assignment across multiple predicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Original_ARG0_index</th>\n",
       "      <th>Original_Predicate_index</th>\n",
       "      <th>Adjusted_ARG0_index</th>\n",
       "      <th>Adjusted_Predicate_index</th>\n",
       "      <th>SpaCy_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John kicked the ball and scored a goal.</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[John, kicked, the, ball, and, scored, a, goal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice painted the canvas and sold the painting.</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[Alice, painted, the, canvas, and, sold, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert cooked dinner and cleaned the kitchen.</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[Robert, cooked, dinner, and, cleaned, the, ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mary read a book and wrote a review.</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[Mary, read, a, book, and, wrote, a, review, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sam drove a car and parked it.</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[Sam, drove, a, car, and, parked, it, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence  Original_ARG0_index  \\\n",
       "0          John kicked the ball and scored a goal.                    0   \n",
       "1  Alice painted the canvas and sold the painting.                    0   \n",
       "2    Robert cooked dinner and cleaned the kitchen.                    0   \n",
       "3             Mary read a book and wrote a review.                    0   \n",
       "4                   Sam drove a car and parked it.                    0   \n",
       "\n",
       "   Original_Predicate_index  Adjusted_ARG0_index  Adjusted_Predicate_index  \\\n",
       "0                         5                    0                         5   \n",
       "1                         5                    0                         5   \n",
       "2                         4                    0                         4   \n",
       "3                         5                    0                         5   \n",
       "4                         5                    0                         5   \n",
       "\n",
       "                                        SpaCy_Tokens  \n",
       "0  [John, kicked, the, ball, and, scored, a, goal...  \n",
       "1  [Alice, painted, the, canvas, and, sold, the, ...  \n",
       "2  [Robert, cooked, dinner, and, cleaned, the, ki...  \n",
       "3    [Mary, read, a, book, and, wrote, a, review, .]  \n",
       "4           [Sam, drove, a, car, and, parked, it, .]  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('distance2.csv')\n",
    "\n",
    "adjusted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row['Sentence']\n",
    "    arg0_index = row['ARG0_Index']\n",
    "    pred_index = row['Last_Predicate_Index']\n",
    "    \n",
    "    adjusted_indices = adjust_indices(sentence, [arg0_index, pred_index])\n",
    "    \n",
    "    adjusted_data.append({\n",
    "        'Sentence': sentence,\n",
    "        'Original_ARG0_index': arg0_index,\n",
    "        'Original_Predicate_index': pred_index,\n",
    "        'Adjusted_ARG0_index': adjusted_indices[0],\n",
    "        'Adjusted_Predicate_index': adjusted_indices[1],\n",
    "        'SpaCy_Tokens': [token.text for token in nlp(sentence)]\n",
    "    })\n",
    "\n",
    "distance2 = pd.DataFrame(adjusted_data)\n",
    "\n",
    "distance2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'restored'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the first row column SpaCy_tokens\n",
    "first_row_spacy_tokens = distance2.iloc[97]['SpaCy_Tokens']\n",
    "first_row_spacy_tokens[5] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 100 examples for capability 'distance' with test type 'MFT' and name '2'\n"
     ]
    }
   ],
   "source": [
    "tokenized_list = [row[\"SpaCy_Tokens\"] for _, row in distance2.iterrows()]\n",
    "predicate_indexes = [row[\"Adjusted_Predicate_index\"] for _, row in distance2.iterrows()]\n",
    "target_indexes = [row[\"Adjusted_ARG0_index\"] for _, row in distance2.iterrows()]\n",
    "target_srls = [\"ARG0\"] * len(tokenized_list)  \n",
    "\n",
    "dataset = add_examples(\n",
    "    dataset,\n",
    "    \"distance\",\n",
    "    \"MFT\",\n",
    "    \"2\",\n",
    "    tokenized_list,\n",
    "    predicate_indexes,\n",
    "    target_indexes,\n",
    "    target_srls\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaceTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIR: Test that switching a modifier from temporal to spatial yields different adjunct roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceTemp</th>\n",
       "      <th>Original_ARGM-TMP_index</th>\n",
       "      <th>Original_Predicate_index</th>\n",
       "      <th>Adjusted_ARGM-TMP_index</th>\n",
       "      <th>Adjusted_Predicate_index</th>\n",
       "      <th>SpaCy_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John met Jack on Friday.</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[John, met, Jack, on, Friday, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mark delivered the package at 3 PM.</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[Mark, delivered, the, package, at, 3, PM, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emma started the presentation at dawn.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[Emma, started, the, presentation, at, dawn, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucas finished the race at sunset.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[Lucas, finished, the, race, at, sunset, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Olivia began the meeting at 10 AM.</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[Olivia, began, the, meeting, at, 10, AM, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             SentenceTemp  Original_ARGM-TMP_index  \\\n",
       "0                John met Jack on Friday.                        4   \n",
       "1     Mark delivered the package at 3 PM.                        6   \n",
       "2  Emma started the presentation at dawn.                        5   \n",
       "3      Lucas finished the race at sunset.                        5   \n",
       "4      Olivia began the meeting at 10 AM.                        6   \n",
       "\n",
       "   Original_Predicate_index  Adjusted_ARGM-TMP_index  \\\n",
       "0                         1                        4   \n",
       "1                         1                        6   \n",
       "2                         1                        5   \n",
       "3                         1                        5   \n",
       "4                         1                        6   \n",
       "\n",
       "   Adjusted_Predicate_index                                     SpaCy_Tokens  \n",
       "0                         1                 [John, met, Jack, on, Friday, .]  \n",
       "1                         1    [Mark, delivered, the, package, at, 3, PM, .]  \n",
       "2                         1  [Emma, started, the, presentation, at, dawn, .]  \n",
       "3                         1      [Lucas, finished, the, race, at, sunset, .]  \n",
       "4                         1     [Olivia, began, the, meeting, at, 10, AM, .]  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spacetemp.csv')\n",
    "\n",
    "adjusted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row['SentenceTemp']\n",
    "    argtemp_index = row['ARGM-TMP_Index']\n",
    "    pred_index = row['Predicate_Index']\n",
    "    \n",
    "    # Adjust indices\n",
    "    adjusted_indices = adjust_indices(sentence, [argtemp_index, pred_index])\n",
    "    \n",
    "    adjusted_data.append({\n",
    "        'SentenceTemp': sentence,\n",
    "        'Original_ARGM-TMP_index': argtemp_index,\n",
    "        'Original_Predicate_index': pred_index,\n",
    "        'Adjusted_ARGM-TMP_index': adjusted_indices[0],\n",
    "        'Adjusted_Predicate_index': adjusted_indices[1],\n",
    "        'SpaCy_Tokens': [token.text for token in nlp(sentence)]\n",
    "    })\n",
    "\n",
    "# Create new DataFrame with adjusted indices\n",
    "temp = pd.DataFrame(adjusted_data)\n",
    "\n",
    "# Display the results\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AM'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the first row column SpaCy_tokens\n",
    "first_row_spacy_tokens = temp.iloc[50]['SpaCy_Tokens']\n",
    "first_row_spacy_tokens[6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list1 = [row[\"SpaCy_Tokens\"] for _, row in  temp.iterrows()]\n",
    "predicate_indexes = [row[\"Adjusted_Predicate_index\"] for _, row in temp.iterrows()]\n",
    "target_index1 = [row[\"Adjusted_ARGM-TMP_index\"] for _, row in temp.iterrows()]\n",
    "target_srls1 = [\"ARGM-TMP\"] * len(tokenized_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceLoc</th>\n",
       "      <th>Original_ARGM-LOC_index</th>\n",
       "      <th>Original_Predicate_index</th>\n",
       "      <th>Adjusted_ARGM-LOC_index</th>\n",
       "      <th>Adjusted_Predicate_index</th>\n",
       "      <th>SpaCy_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John met Jack under a bridge.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[John, met, Jack, under, a, bridge, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mark delivered the package at the post office.</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[Mark, delivered, the, package, at, the, post,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emma started the presentation near the auditor...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[Emma, started, the, presentation, near, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucas finished the race on the track.</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[Lucas, finished, the, race, on, the, track, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Olivia began the meeting at the conference room.</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[Olivia, began, the, meeting, at, the, confere...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         SentenceLoc  Original_ARGM-LOC_index  \\\n",
       "0                      John met Jack under a bridge.                        5   \n",
       "1     Mark delivered the package at the post office.                        7   \n",
       "2  Emma started the presentation near the auditor...                        6   \n",
       "3              Lucas finished the race on the track.                        6   \n",
       "4   Olivia began the meeting at the conference room.                        7   \n",
       "\n",
       "   Original_Predicate_index  Adjusted_ARGM-LOC_index  \\\n",
       "0                         1                        5   \n",
       "1                         1                        7   \n",
       "2                         1                        6   \n",
       "3                         1                        6   \n",
       "4                         1                        7   \n",
       "\n",
       "   Adjusted_Predicate_index                                       SpaCy_Tokens  \n",
       "0                         1             [John, met, Jack, under, a, bridge, .]  \n",
       "1                         1  [Mark, delivered, the, package, at, the, post,...  \n",
       "2                         1  [Emma, started, the, presentation, near, the, ...  \n",
       "3                         1    [Lucas, finished, the, race, on, the, track, .]  \n",
       "4                         1  [Olivia, began, the, meeting, at, the, confere...  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spacetemp.csv')\n",
    "\n",
    "adjusted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row['SentenceLoc']\n",
    "    argloc_index = row['ARGM-LOC_Index']\n",
    "    pred_index = row['Predicate_Index']\n",
    "    \n",
    "    # Adjust indices\n",
    "    adjusted_indices = adjust_indices(sentence, [argloc_index, pred_index])\n",
    "    \n",
    "    adjusted_data.append({\n",
    "        'SentenceLoc': sentence,\n",
    "        'Original_ARGM-LOC_index': argloc_index,\n",
    "        'Original_Predicate_index': pred_index,\n",
    "        'Adjusted_ARGM-LOC_index': adjusted_indices[0],\n",
    "        'Adjusted_Predicate_index': adjusted_indices[1],\n",
    "        'SpaCy_Tokens': [token.text for token in nlp(sentence)]\n",
    "    })\n",
    "\n",
    "# Create new DataFrame with adjusted indices\n",
    "space = pd.DataFrame(adjusted_data)\n",
    "\n",
    "# Display the results\n",
    "space.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list2 = [row[\"SpaCy_Tokens\"] for _, row in space .iterrows()]\n",
    "target_index2 = [row[\"Adjusted_ARGM-LOC_index\"] for _, row in space.iterrows()]\n",
    "target_srls2 = [\"ARGM-LOC\"] * len(tokenized_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 59 examples for capability 'spacetemp' with test type 'DIR' and name '1'\n"
     ]
    }
   ],
   "source": [
    "dataset = add_examples(\n",
    "    dataset,\n",
    "    \"spacetemp\",\n",
    "    \"DIR\",\n",
    "    \"1\",\n",
    "    tokenized_list1,\n",
    "    tokenized_list2,\n",
    "    predicate_indexes,\n",
    "    target_index1,\n",
    "    target_index2,\n",
    "    target_srls1,\n",
    "    target_srls2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DativeAlter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INV: Verify that active and prepositional constructions consistently label the recipient (indirect object) as ARG2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceActive</th>\n",
       "      <th>Original_ARG2_index</th>\n",
       "      <th>Original_Predicate_index</th>\n",
       "      <th>Adjusted_ARG2_index</th>\n",
       "      <th>Adjusted_Predicate_index</th>\n",
       "      <th>SpaCy_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I gave John a book.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, gave, John, a, book, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She sent Mary a letter.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[She, sent, Mary, a, letter, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He offered Tom a ride.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[He, offered, Tom, a, ride, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They handed Mark a gift.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[They, handed, Mark, a, gift, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We provided Jane a chance.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[We, provided, Jane, a, chance, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SentenceActive  Original_ARG2_index  Original_Predicate_index  \\\n",
       "0         I gave John a book.                    2                         1   \n",
       "1     She sent Mary a letter.                    2                         1   \n",
       "2      He offered Tom a ride.                    2                         1   \n",
       "3    They handed Mark a gift.                    2                         1   \n",
       "4  We provided Jane a chance.                    2                         1   \n",
       "\n",
       "   Adjusted_ARG2_index  Adjusted_Predicate_index  \\\n",
       "0                    2                         1   \n",
       "1                    2                         1   \n",
       "2                    2                         1   \n",
       "3                    2                         1   \n",
       "4                    2                         1   \n",
       "\n",
       "                         SpaCy_Tokens  \n",
       "0         [I, gave, John, a, book, .]  \n",
       "1     [She, sent, Mary, a, letter, .]  \n",
       "2      [He, offered, Tom, a, ride, .]  \n",
       "3    [They, handed, Mark, a, gift, .]  \n",
       "4  [We, provided, Jane, a, chance, .]  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dative.csv')\n",
    "\n",
    "adjusted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row['SentenceActive']\n",
    "    arg2_index = row['Active_ARG2_Index']\n",
    "    pred_index = row['Predicate_Index']\n",
    "    \n",
    "    # Adjust indices\n",
    "    adjusted_indices = adjust_indices(sentence, [arg2_index, pred_index])\n",
    "    \n",
    "    adjusted_data.append({\n",
    "        'SentenceActive': sentence,\n",
    "        'Original_ARG2_index': arg2_index,\n",
    "        'Original_Predicate_index': pred_index,\n",
    "        'Adjusted_ARG2_index': adjusted_indices[0],\n",
    "        'Adjusted_Predicate_index': adjusted_indices[1],\n",
    "        'SpaCy_Tokens': [token.text for token in nlp(sentence)]\n",
    "    })\n",
    "\n",
    "# Create new DataFrame with adjusted indices\n",
    "dative_active = pd.DataFrame(adjusted_data)\n",
    "\n",
    "# Display the results\n",
    "dative_active.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list1 = [row[\"SpaCy_Tokens\"] for _, row in  dative_active.iterrows()]\n",
    "predicate_indexes = [row[\"Adjusted_Predicate_index\"] for _, row in dative_active.iterrows()]\n",
    "target_index1 = [row[\"Adjusted_ARG2_index\"] for _, row in dative_active.iterrows()]\n",
    "target_srls = [\"ARG2\"] * len(tokenized_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentencePrepositional</th>\n",
       "      <th>Original_ARG2_index</th>\n",
       "      <th>Original_Predicate_index</th>\n",
       "      <th>Adjusted_ARG2_index</th>\n",
       "      <th>Adjusted_Predicate_index</th>\n",
       "      <th>SpaCy_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I gave a book to John.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, gave, a, book, to, John, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She sent a letter to Mary.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[She, sent, a, letter, to, Mary, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He offered a ride to Tom.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[He, offered, a, ride, to, Tom, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They handed a gift to Mark.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[They, handed, a, gift, to, Mark, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We provided a chance to Jane.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[We, provided, a, chance, to, Jane, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SentencePrepositional  Original_ARG2_index  \\\n",
       "0         I gave a book to John.                    5   \n",
       "1     She sent a letter to Mary.                    5   \n",
       "2      He offered a ride to Tom.                    5   \n",
       "3    They handed a gift to Mark.                    5   \n",
       "4  We provided a chance to Jane.                    5   \n",
       "\n",
       "   Original_Predicate_index  Adjusted_ARG2_index  Adjusted_Predicate_index  \\\n",
       "0                         1                    5                         1   \n",
       "1                         1                    5                         1   \n",
       "2                         1                    5                         1   \n",
       "3                         1                    5                         1   \n",
       "4                         1                    5                         1   \n",
       "\n",
       "                             SpaCy_Tokens  \n",
       "0         [I, gave, a, book, to, John, .]  \n",
       "1     [She, sent, a, letter, to, Mary, .]  \n",
       "2      [He, offered, a, ride, to, Tom, .]  \n",
       "3    [They, handed, a, gift, to, Mark, .]  \n",
       "4  [We, provided, a, chance, to, Jane, .]  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dative.csv')\n",
    "\n",
    "adjusted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row['SentencePrepositional']\n",
    "    arg2_index = row['Prepositional_ARG2_Index']\n",
    "    pred_index = row['Predicate_Index']\n",
    "    \n",
    "    # Adjust indices\n",
    "    adjusted_indices = adjust_indices(sentence, [arg2_index, pred_index])\n",
    "    \n",
    "    adjusted_data.append({\n",
    "        'SentencePrepositional': sentence,\n",
    "        'Original_ARG2_index': arg2_index,\n",
    "        'Original_Predicate_index': pred_index,\n",
    "        'Adjusted_ARG2_index': adjusted_indices[0],\n",
    "        'Adjusted_Predicate_index': adjusted_indices[1],\n",
    "        'SpaCy_Tokens': [token.text for token in nlp(sentence)]\n",
    "    })\n",
    "\n",
    "# Create new DataFrame with adjusted indices\n",
    "dative_prep = pd.DataFrame(adjusted_data)\n",
    "\n",
    "# Display the results\n",
    "dative_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list2 = [row[\"SpaCy_Tokens\"] for _, row in  dative_prep.iterrows()]\n",
    "target_index2 = [row[\"Adjusted_ARG2_index\"] for _, row in dative_prep.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 57 examples for capability 'dativealter' with test type 'INV' and name '1'\n"
     ]
    }
   ],
   "source": [
    "dataset = add_examples(\n",
    "    dataset,\n",
    "    \"dativealter\",\n",
    "    \"INV\",\n",
    "    \"1\",\n",
    "    tokenized_list1,\n",
    "    tokenized_list2,\n",
    "    predicate_indexes,\n",
    "    target_index1,\n",
    "    target_index2,\n",
    "    target_srls\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Original_Predicate_index</th>\n",
       "      <th>Original_ARGM_NEG_index</th>\n",
       "      <th>Adjusted_Predicate_index</th>\n",
       "      <th>Adjusted_ARGM_NEG_index</th>\n",
       "      <th>SpaCy_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jack did not clean the house.</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[Jack, did, not, clean, the, house, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maria does not like spicy food.</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[Maria, does, not, like, spicy, food, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The manager can not approve the proposal.</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>[The, manager, can, not, approve, the, proposa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He will not attend the meeting tomorrow.</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[He, will, not, attend, the, meeting, tomorrow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We can not find any evidence of the claim.</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[We, can, not, find, any, evidence, of, the, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Sentence  Original_Predicate_index  \\\n",
       "0               Jack did not clean the house.                         3   \n",
       "1             Maria does not like spicy food.                         3   \n",
       "2   The manager can not approve the proposal.                         4   \n",
       "3    He will not attend the meeting tomorrow.                         3   \n",
       "4  We can not find any evidence of the claim.                         3   \n",
       "\n",
       "   Original_ARGM_NEG_index  Adjusted_Predicate_index  Adjusted_ARGM_NEG_index  \\\n",
       "0                        2                         3                        2   \n",
       "1                        2                         3                        2   \n",
       "2                        3                         4                        3   \n",
       "3                        2                         3                        2   \n",
       "4                        2                         3                        2   \n",
       "\n",
       "                                        SpaCy_Tokens  \n",
       "0             [Jack, did, not, clean, the, house, .]  \n",
       "1           [Maria, does, not, like, spicy, food, .]  \n",
       "2  [The, manager, can, not, approve, the, proposa...  \n",
       "3  [He, will, not, attend, the, meeting, tomorrow...  \n",
       "4  [We, can, not, find, any, evidence, of, the, c...  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('negation.csv')\n",
    "\n",
    "adjusted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row['Sentence']\n",
    "    pred_index = row['Predicate_Index']\n",
    "    neg_index = row['ARGM_NEG_Index']\n",
    "\n",
    "    adjusted_indices = adjust_indices(sentence, [pred_index, neg_index])\n",
    "    \n",
    "    adjusted_data.append({\n",
    "        'Sentence': sentence,\n",
    "        'Original_Predicate_index': pred_index,\n",
    "        'Original_ARGM_NEG_index': neg_index,\n",
    "        'Adjusted_Predicate_index': adjusted_indices[0],\n",
    "        'Adjusted_ARGM_NEG_index': adjusted_indices[1],\n",
    "        'SpaCy_Tokens': [token.text for token in nlp(sentence)]\n",
    "    })\n",
    "\n",
    "negation = pd.DataFrame(adjusted_data)\n",
    "\n",
    "negation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 50 examples for capability 'negation' with test type 'MFT' and name '1'\n"
     ]
    }
   ],
   "source": [
    "tokenized_list = [row[\"SpaCy_Tokens\"] for _, row in negation.iterrows()]\n",
    "predicate_indexes = [row[\"Adjusted_Predicate_index\"] for _, row in negation.iterrows()]\n",
    "target_indexes = [row[\"Adjusted_ARGM_NEG_index\"] for _, row in negation.iterrows()]\n",
    "target_srls = [\"ARGM-NEG\"] * len(tokenized_list)  \n",
    "\n",
    "dataset = add_examples(\n",
    "    dataset,\n",
    "    \"negation\",\n",
    "    \"MFT\",\n",
    "    \"1\",\n",
    "    tokenized_list,\n",
    "    predicate_indexes,\n",
    "    target_indexes,\n",
    "    target_srls\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFT: Test that the model correctly identifies the head noun from a noun phrase with a single (participial) adjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Original_Predicate_index</th>\n",
       "      <th>Original_ARG0_index</th>\n",
       "      <th>Adjusted_Predicate_index</th>\n",
       "      <th>Adjusted_ARG0_index</th>\n",
       "      <th>SpaCy_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The red car zoomed down the highway.</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[The, red, car, zoomed, down, the, highway, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tall buildings collapsed suddenly.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[Tall, buildings, collapsed, suddenly, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The ancient tree fell in the storm.</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[The, ancient, tree, fell, in, the, storm, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A broken vase shattered on the floor.</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, broken, vase, shattered, on, the, floor, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sleepy cats purred softly.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[Sleepy, cats, purred, softly, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Sentence  Original_Predicate_index  \\\n",
       "0   The red car zoomed down the highway.                         3   \n",
       "1     Tall buildings collapsed suddenly.                         2   \n",
       "2    The ancient tree fell in the storm.                         3   \n",
       "3  A broken vase shattered on the floor.                         3   \n",
       "4             Sleepy cats purred softly.                         2   \n",
       "\n",
       "   Original_ARG0_index  Adjusted_Predicate_index  Adjusted_ARG0_index  \\\n",
       "0                    2                         3                    2   \n",
       "1                    1                         2                    1   \n",
       "2                    2                         3                    2   \n",
       "3                    2                         3                    2   \n",
       "4                    1                         2                    1   \n",
       "\n",
       "                                      SpaCy_Tokens  \n",
       "0   [The, red, car, zoomed, down, the, highway, .]  \n",
       "1        [Tall, buildings, collapsed, suddenly, .]  \n",
       "2    [The, ancient, tree, fell, in, the, storm, .]  \n",
       "3  [A, broken, vase, shattered, on, the, floor, .]  \n",
       "4                [Sleepy, cats, purred, softly, .]  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('head1.csv')\n",
    "\n",
    "adjusted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row['Sentence']\n",
    "    pred_index = row['Predicate_Index']\n",
    "    arg0_index = row['ARG0_Index']\n",
    "\n",
    "    adjusted_indices = adjust_indices(sentence, [pred_index, arg0_index])\n",
    "    \n",
    "    adjusted_data.append({\n",
    "        'Sentence': sentence,\n",
    "        'Original_Predicate_index': pred_index,\n",
    "        'Original_ARG0_index': arg0_index,\n",
    "        'Adjusted_Predicate_index': adjusted_indices[0],\n",
    "        'Adjusted_ARG0_index': adjusted_indices[1],\n",
    "        'SpaCy_Tokens': [token.text for token in nlp(sentence)]\n",
    "    })\n",
    "\n",
    "head1 = pd.DataFrame(adjusted_data)\n",
    "head1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 50 examples for capability 'head' with test type 'MFT' and name '1'\n"
     ]
    }
   ],
   "source": [
    "tokenized_list = [row[\"SpaCy_Tokens\"] for _, row in head1.iterrows()]\n",
    "predicate_indexes = [row[\"Adjusted_Predicate_index\"] for _, row in head1.iterrows()]\n",
    "target_indexes = [row[\"Adjusted_ARG0_index\"] for _, row in head1.iterrows()]\n",
    "target_srls = [\"ARG0\"] * len(tokenized_list)  \n",
    "\n",
    "dataset = add_examples(\n",
    "    dataset,\n",
    "    \"head\",\n",
    "    \"MFT\",\n",
    "    \"1\",\n",
    "    tokenized_list,\n",
    "    predicate_indexes,\n",
    "    target_indexes,\n",
    "    target_srls\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFT: Test that the model correctly identifies the head noun from a noun phrase with multiple (participial) adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Original_Predicate_index</th>\n",
       "      <th>Original_ARG0_index</th>\n",
       "      <th>Adjusted_Predicate_index</th>\n",
       "      <th>Adjusted_ARG0_index</th>\n",
       "      <th>SpaCy_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The old, abandoned building fell.</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>[The, old, ,, abandoned, building, fell, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The bright, shining star twinkled in the night.</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>[The, bright, ,, shining, star, twinkled, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The large, rumpled suitcase arrived at the sta...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>[The, large, ,, rumpled, suitcase, arrived, at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The green, overgrown garden flourished in spring.</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>[The, green, ,, overgrown, garden, flourished,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The elegant, Victorian mansion stood silently ...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>[The, elegant, ,, Victorian, mansion, stood, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0                  The old, abandoned building fell.   \n",
       "1    The bright, shining star twinkled in the night.   \n",
       "2  The large, rumpled suitcase arrived at the sta...   \n",
       "3  The green, overgrown garden flourished in spring.   \n",
       "4  The elegant, Victorian mansion stood silently ...   \n",
       "\n",
       "   Original_Predicate_index  Original_ARG0_index  Adjusted_Predicate_index  \\\n",
       "0                         4                    3                         5   \n",
       "1                         4                    3                         5   \n",
       "2                         4                    3                         5   \n",
       "3                         4                    3                         5   \n",
       "4                         4                    3                         5   \n",
       "\n",
       "   Adjusted_ARG0_index                                       SpaCy_Tokens  \n",
       "0                    4        [The, old, ,, abandoned, building, fell, .]  \n",
       "1                    4  [The, bright, ,, shining, star, twinkled, in, ...  \n",
       "2                    4  [The, large, ,, rumpled, suitcase, arrived, at...  \n",
       "3                    4  [The, green, ,, overgrown, garden, flourished,...  \n",
       "4                    4  [The, elegant, ,, Victorian, mansion, stood, s...  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('head2.csv')\n",
    "\n",
    "adjusted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row['Sentence']\n",
    "    pred_index = row['Predicate_Index']\n",
    "    arg0_index = row['ARG0_Index']\n",
    "\n",
    "    adjusted_indices = adjust_indices(sentence, [pred_index, arg0_index])\n",
    "    \n",
    "    adjusted_data.append({\n",
    "        'Sentence': sentence,\n",
    "        'Original_Predicate_index': pred_index,\n",
    "        'Original_ARG0_index': arg0_index,\n",
    "        'Adjusted_Predicate_index': adjusted_indices[0],\n",
    "        'Adjusted_ARG0_index': adjusted_indices[1],\n",
    "        'SpaCy_Tokens': [token.text for token in nlp(sentence)]\n",
    "    })\n",
    "\n",
    "head2 = pd.DataFrame(adjusted_data)\n",
    "head2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 50 examples for capability 'head' with test type 'MFT' and name '2'\n"
     ]
    }
   ],
   "source": [
    "tokenized_list = [row[\"SpaCy_Tokens\"] for _, row in head2.iterrows()]\n",
    "predicate_indexes = [row[\"Adjusted_Predicate_index\"] for _, row in head2.iterrows()]\n",
    "target_indexes = [row[\"Adjusted_ARG0_index\"] for _, row in head2.iterrows()]\n",
    "target_srls = [\"ARG0\"] * len(tokenized_list)  \n",
    "\n",
    "dataset = add_examples(\n",
    "    dataset,\n",
    "    \"head\",\n",
    "    \"MFT\",\n",
    "    \"2\",\n",
    "    tokenized_list,\n",
    "    predicate_indexes,\n",
    "    target_indexes,\n",
    "    target_srls\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambigous Predicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually select predicate positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_arg=[\n",
    "    \"He assembled enemies into a united group.\",\n",
    "    \"The dog heeled to me in obedience.\",\n",
    "    \"John forwent to the circus.\",\n",
    "    \"She picked at her pasta from the plate.\",\n",
    "    \"John stresses about environmental awareness.\",\n",
    "    \"We cut a truce with them for peace.\"\n",
    "]\n",
    "\n",
    "with_arg=[\n",
    "    \"John's assembly of the furniture from a kit to a home.\",\n",
    "    \"He heeled the bike over.\",\n",
    "    \"Benefits are forgone to the private sector.\",\n",
    "    \"You can't pick grapes from thornbushes.\",\n",
    "    \"He was stressed about the test.\",\n",
    "    \"Google cut its price to five dollars.\"\n",
    "]\n",
    "\n",
    "SRL_oi=[\"ARG4\", \"ARG2\", \"ARG3\", \"ARG2\", \"ARG3\", \"ARG4\"]\n",
    "\n",
    "tokenized_without_arg = [[token.text for token in nlp(sentence)] for sentence in without_arg]\n",
    "tokenized_with_arg = [[token.text for token in nlp(sentence)] for sentence in with_arg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['He', 'assembled', 'enemies', 'into', 'a', 'united', 'group', '.'], ['The', 'dog', 'heeled', 'to', 'me', 'in', 'obedience', '.'], ['John', 'forwent', 'to', 'the', 'circus', '.'], ['She', 'picked', 'at', 'her', 'pasta', 'from', 'the', 'plate', '.'], ['John', 'stresses', 'about', 'environmental', 'awareness', '.'], ['We', 'cut', 'a', 'truce', 'with', 'them', 'for', 'peace', '.']]\n",
      "[['John', \"'s\", 'assembly', 'of', 'the', 'furniture', 'from', 'a', 'kit', 'to', 'a', 'home', '.'], ['He', 'heeled', 'the', 'bike', 'over', '.'], ['Benefits', 'are', 'forgone', 'to', 'the', 'private', 'sector', '.'], ['You', 'ca', \"n't\", 'pick', 'grapes', 'from', 'thornbushes', '.'], ['He', 'was', 'stressed', 'about', 'the', 'test', '.'], ['Google', 'cut', 'its', 'price', 'to', 'five', 'dollars', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_without_arg)\n",
    "print(tokenized_with_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate1=[1, 2, 1, 1, 1, 1]\n",
    "predicate2=[2, 1, 2, 3, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6 examples for capability 'predicatedis' with test type 'specialDIR' and name '1'\n"
     ]
    }
   ],
   "source": [
    "dataset = add_examples(\n",
    "    dataset,\n",
    "    \"predicatedis\",\n",
    "    \"specialDIR\",\n",
    "    \"1\",\n",
    "    tokenized_without_arg,\n",
    "    tokenized_with_arg,\n",
    "    predicate1,\n",
    "    predicate2,\n",
    "    SRL_oi\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.json', 'w') as json_file:\n",
    "    json.dump(dataset, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
